## 一、连续型随机变量的期望与方差

对于随机变量$X$，用$f_X(x)$表示其**概率密度函数**

>连续型随机变量的概率密度函数（可简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数
>
>随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分

### 1.1 期望

**随机变量$X$的期望（expected value）定义为：**
$$
E(X) = \int_{-\infty}^{\infty} x f_X(x) dx
$$

表示对随机变量的数值进行**平均运算**

期望的运算是**线性运算**，存在以下重要性质：
$$
E(a) = a \\
E(aX) = aE(X)
$$

- a是常数

$$
E(X+Y) = E(X) + E(Y)
$$

- X和Y是两个随机变量

$$
E(XY) = E(X)E(Y)
$$

- X和Y是两个**相互独立**的随机变量

### 1.2 方差

 **随机变量$X$的方差（variance）定义为：**
$$
Var(X) = \int_{-\infty}^{\infty} [X-E(X)]^2 f_X(x) dx
$$
根据期望的定义可将方差的定义写成：
$$
Var(X) = E((X-E(X))^2)
$$
**即，X的方差就是( X - E(X) )^2的期望**

**方差描述了随机变量X的离散程度**，方差小表示随机变量的概率密度集中在期望E(X)附近。根据期望的线性性质还可得到：
$$
Var(X) = E(X^2) - E(X)^2
$$
方差的性质：
$$
Var(a) = 0 \\
Var(aX) = a^2 Var(X)
$$

- a是常数

$$
Var(X+Y) = Var(X) + 2E(X-E(X))(Y-E(Y)) + Var(Y)
$$

- X和Y是两个随机变量

$$
Var(X+Y) = Var(X) + Var(Y)
$$

- X和Y是两个**相互独立**的随机变量

### 1.3 标准差

标准差（standard deviation）是**方差的算术平方根**，定义为：
$$
\sigma_X = \sqrt{Var(X)}
$$
因此，随机变量X的方差可以写成：
$$
Var(X) = \sigma_X^2
$$


## 二、正态分布

正态分布（normal distribution）也称为高斯分布，是自然界普遍存在的一种分布方式，大部分随机过程中产生的误差都服从或者接近于正态分布。若一个随机变量X服从正态分布，可表达为：
$$
X \sim N(\mu, \sigma^2)
$$

- $\mu = E(X)$ 代表期望
- $\sigma^2 = Var(X)$ 代表方差

正态分布的概率密度在图中呈一条**钟形**曲线，集中在期望附近

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202404151536431.png" alt="image-20240415153654354" style="zoom:50%;" />
$$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

## 三、协方差和协方差矩阵

当两个传感器之间是相互独立（即一个传感器的测量结果不影响另外一个传感器的测量结果）的，在融合时只需要考虑各自的方差即可。但在某些情况下，两个信号之间存在关联（例如一个信号会放大另外一个信号），在融合时就需要考虑两者之间的关联

### 3.1 统计学角度

两组信号之间的联动关系使用**协方差（covariance）**表示，它可以用于衡量两组变量的**联合变化程度**
$$
\sigma_{x_ix_j} = \frac{1}{n-1} \sum_{i=1}^{n}(x_{i_m}-\bar{x}_i)(x_{2_j}-\bar{x}_j)
$$
且有：$\sigma_{x_ix_j} = \sigma_{x_jx_i}$

- 协方差绝对值大则表明相关性强，反之则相关性弱

当有多组信号时，可用**协方差矩阵（covariance matrix）**表示
$$
C(x_1,x_2,x_3) = \begin{bmatrix} 
					\sigma_{x_1}^2 & \sigma_{x_1x_2} & \sigma_{x_1x_3} \\
					\sigma_{x_2x_1} & \sigma_{x_2}^2 & \sigma_{x_2x_3} \\
					\sigma_{x_3x_1} & \sigma_{x_3x_2} & \sigma_{x_3}^2
				 \end{bmatrix}
$$

- 协方差矩阵是一个**方阵**，维度与样本个数一致
- 对角线元素是单个变量的方差，其余位置是两两不同变量之间的协方差
- 协方差矩阵是**对称**的，即$C(x_1,x_2,x_3) = C^T(x_1,x_2,x_3)$

对于一组统计数据，协方差矩阵可以简洁明确地给出每个变量之间的联动关系以及变量自身的离散程度

### 3.2 随机变量之间的协方差与协方差矩阵

考虑两个随机变量$X_1$和$X_2$，它们之间的协方差定义为：
$$
\sigma_{X_1X_2} = Cov(X_1, X_2) = E( (X_1-E(X_1))(X_2-E(X_2)) ) = E(X_1X_2)-E(X_1)E(X_2)
$$

- 当$X_1 = X_2 = X$时，则有$Cov(X, X) = E(X^2) - E(X)^2 = Var(X)$，即**方差就说一个随机变量与其自身的协方差**
- **如果$X_1$和$X_2$相互独立**，则有$E(X_1X_2) = E(X_1)E(X_2)$，那么有$Cov(X_1, X_2) = 0$，即**两个相互独立的随机变量之间没有联动关系，协方差为0**

向量$X$是一个由n个随机变量组成的$n \times 1$向量，且其中每一个随机变量$X_i$都服从正态分布，即$X_i \sim N(\mu_{X_i},\sigma_{X_i}^2)$，则$X$的期望即可定义为：$E(X) = \mu_{X}$ ，$\mu_{X}$是一个$n \times 1$向量。则$X$的协方差矩阵可定义为：
$$
C(X) = E[(X-\mu_X)(X-\mu_X)^T] = \begin{bmatrix} 
					\sigma_{X_1}^2 & \cdots & \sigma_{X_1x_n} \\
					\vdots & \ddots & \vdots \\
					\sigma_{x_nx_1} & \cdots & \sigma_{x_n}^2
				 \end{bmatrix}
$$
<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202404152115496.png" alt="image-20240415211533355" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Jian-wei-peng/typora-pic/main/202404152119568.png" alt="image-20240415211923419" style="zoom: 34%;" />









